{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Danicoder/Especializacion_IA_BigData/blob/main/UT%207-2%20LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DANIELA GARCÍA MILLÁN"
      ],
      "metadata": {
        "id": "geNkvRrNrVLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q  einops accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "T2pmZKd5qdaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "usFaNnOBqddE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # PyTorch\n",
        "import numpy as np\n",
        "import getpass\n",
        "import os\n",
        "import transformers\n",
        "import accelerate\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "jLW8nIuVqdfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(transformers.__version__)\n",
        "print(torch.__version__)\n",
        "print(accelerate.__version__)"
      ],
      "metadata": {
        "id": "6V36dqN7qdie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "S5vN1fMHqdlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(42)"
      ],
      "metadata": {
        "id": "oENlRxAXqdoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HF_TOKEN\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "rJlHratlqdqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#id_model = \"microsoft/Phi-3-mini-4k-instruct\"  # no funciona\n",
        "\n",
        "id_model = \"microsoft/Phi-4-mini-instruct\"\n"
      ],
      "metadata": {
        "id": "b5sFSTArqdtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = AutoModelForCausalLM.from_pretrained(id_model, device_map = \"cuda\", torch_dtype = \"auto\",\n",
        "                                             trust_remote_code = True, attn_implementation = \"eager\")"
      ],
      "metadata": {
        "id": "OJUwNaTIqdwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(id_model)"
      ],
      "metadata": {
        "id": "VBRg0wuhqdzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generacion = pipeline(\"text-generation\", model = model1, tokenizer = tokenizer)"
      ],
      "metadata": {
        "id": "jQELJtZDqd2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_args = {\n",
        "    \"max_new_tokens\": 500,\n",
        "    \"return_full_text\": False,\n",
        "    \"temperature\": 0.1, # from 0.1 to 0.9\n",
        "    \"do_sample\": True,\n",
        "}"
      ],
      "metadata": {
        "id": "OickX7CSqd4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt = \"¿Qué es la inteligencia artificial general?\"\n",
        "prompt = \"¿Como se hace una tortilla española con pocas palabras?\"\n",
        "#prompt= \"Erase una vez un instituto de educacion secundaria donde \"\n",
        "output = generacion(prompt, **generation_args)"
      ],
      "metadata": {
        "id": "OMSWaJ8Oqd7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "id": "kiTJ-3-zqd-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"what is the result of 7 x 6 - 42?\"\n",
        "output = generacion(prompt, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "RC7irvecqeBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Who was the first woman in space?\"\n",
        "output = generacion(prompt, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "uoYYw7mAqeD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Una red cnn es \""
      ],
      "metadata": {
        "id": "8h7ppAArqeGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"<|system|>\n",
        "You are a helpful assistant and always respond in spanish<|end|>\n",
        "<|user|>\n",
        "\"{}\"<|end|>\n",
        "<|assistant|>\"\"\".format(prompt)"
      ],
      "metadata": {
        "id": "q5QO6ZpmqeJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template"
      ],
      "metadata": {
        "id": "xmppQmfUqeLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = generacion(template, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "2evs-EOAqeOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Cual fue la primera mujer en el espacio?\" # @param {type: \"string\"}\n",
        "\n",
        "template = \"\"\"<|system|>\n",
        "You are a helpful assistant.<|end|>\n",
        "<|user|>\n",
        "\"{}\"<|end|>\n",
        "<|assistant|>\"\"\".format(prompt)\n",
        "\n",
        "output = generacion(template, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "C0xfwvDGq98Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is AI? Answer in 1 sentence in Spanish in form of a joke \" # @param {type:\"string\"}\n",
        "#prompt = \"What is AI? Answer in 1 sentence\" # @param {type:\"string\"}\n",
        "#prompt = \"What is AI? Answer in the form of a poem\" # @param {type: \"string\"}\n",
        "\n",
        "sys_prompt = \"You are a helpful virtual assistant.\"\n",
        "\n",
        "template = \"\"\"<|system|>\n",
        "{}<|end|>\n",
        "<|user|>\n",
        "\"{}\"<|end|>\n",
        "<|assistant|>\"\"\".format(sys_prompt, prompt)\n",
        "\n",
        "print(template)\n",
        "\n",
        "output = generacion(template, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "lY-T3RMyq95z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Generate a python code that writes the fibonnaci sequence\"\n",
        "\n",
        "sys_prompt = \"You are an experienced programmer. Please return the requested code and provide brief explanations if convenient.\"\n",
        "\n",
        "template = \"\"\"<|system|>\n",
        "{}<|end|>\n",
        "<|user|>\n",
        "\"{}\"<|end|>\n",
        "<|assistant|>\"\"\".format(sys_prompt, prompt)\n",
        "\n",
        "output = generacion(template, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "oYRXeVscq93H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fibonacci(n):\n",
        "    # Initialize the first two Fibonacci numbers\n",
        "    fib_sequence = [0, 1]\n",
        "\n",
        "    # Generate the Fibonacci sequence up to n terms\n",
        "    for i in range(2, n):\n",
        "        next_fib = fib_sequence[i-1] + fib_sequence[i-2]\n",
        "        fib_sequence.append(next_fib)\n",
        "\n",
        "    return fib_sequence\n",
        "\n",
        "# Specify the number of terms in the Fibonacci sequence\n",
        "num_terms = 10\n",
        "\n",
        "# Generate and print the Fibonacci sequence\n",
        "fib_sequence = fibonacci(num_terms)\n",
        "print(fib_sequence)"
      ],
      "metadata": {
        "id": "aahTTZj2q901"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fibonacci(n):\n",
        "    \"\"\"\n",
        "    Generate the Fibonacci sequence up to n terms.\n",
        "\n",
        "    Parameters:\n",
        "    n (int): The number of terms to generate.\n",
        "\n",
        "    Returns:\n",
        "    list: A list containing the Fibonacci sequence up to n terms.\n",
        "    \"\"\"\n",
        "    fib_sequence = [0, 1]\n",
        "    if n <= 0:\n",
        "        return []\n",
        "    elif n == 1:\n",
        "        return [0]\n",
        "    elif n == 2:\n",
        "        return fib_sequence\n",
        "\n",
        "    for i in range(2, n):\n",
        "        next_term = fib_sequence[i-1] + fib_sequence[i-2]\n",
        "        fib_sequence.append(next_term)\n",
        "\n",
        "    return fib_sequence\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "n = 10\n",
        "print(f\"Fibonacci sequence up to {n} terms: {fibonacci(n)}\")"
      ],
      "metadata": {
        "id": "mOuKWPcyq9yW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"¿Que es IA generativa?\"\n",
        "\n",
        "msg = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful virtual assistant in data science.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "\n",
        "output = generacion(msg, **generation_args)\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "yOniA_VSq9vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"List 10 famous cities in Spain\"\n",
        "prompt_sys = \"You are a helpful travel assistant.\"\n",
        "\n",
        "msg = [\n",
        "    {\"role\": \"system\", \"content\": prompt_sys},\n",
        "    {\"role\": \"user\", \"content\": prompt},\n",
        "]\n",
        "\n",
        "output = generacion(msg, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "u5eOAKSeq9s5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit = True,\n",
        "    bnb_4bit_quant_type = \"nf4\",\n",
        "    bnb_4bit_use_double_quant = True,\n",
        "    bnb_4bit_compute_dtype = torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "18-MtUojq9p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "model2 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config = quantization_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "jXgRAVAnq9na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = (\"Who was the first person in space?\")\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]"
      ],
      "metadata": {
        "id": "Lq8rlB0Sq9k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\") # PyTorch\n",
        "model_inputs = encodeds.to(device)\n",
        "generated_ids = model2.generate(model_inputs, max_new_tokens = 1000, do_sample = True,\n",
        "                               pad_token_id = tokenizer.eos_token_id)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "res = decoded[0]"
      ],
      "metadata": {
        "id": "RcsblXP6q9ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "id": "ItEz2ro1q9f6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Te damos la bienvenida a Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}